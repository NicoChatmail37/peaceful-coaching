export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMRequest {
  model: string;
  messages: LLMMessage[];
  temperature?: number;
  stream?: boolean;
  max_tokens?: number;
}

export interface LLMResponse {
  choices: Array<{
    message: {
      content: string;
    };
    finish_reason: string;
  }>;
}

export interface LLMStreamChunk {
  choices: Array<{
    delta: {
      content?: string;
    };
    finish_reason?: string;
  }>;
}

export interface LLMBridgeStatus {
  isConnected: boolean;
  backend: 'ollama' | 'lmstudio' | 'disabled';
  models: string[];
  error?: string;
  url?: string;
}

export type AnalysisType = 'summary' | 'todos' | 'notes' | 'custom';

export interface AnalysisResult {
  type: AnalysisType;
  prompt: string;
  result: string;
  model: string;
  created_at: string;
}

const LLM_BRIDGE_URL = 'http://localhost:27123';

// Templates de prompts optimis√©s pour le contexte m√©dical/th√©rapeutique
export const PROMPT_TEMPLATES = {
  summary: `Tu es un assistant de transcription m√©dicale. Analysez cette consultation et produisez un r√©sum√© structur√© en 5 points cl√©s :

1. **Motif de consultation** : Raison principale de la visite
2. **Observations cliniques** : √âtat du patient, sympt√¥mes observ√©s  
3. **√âvaluation/Diagnostic** : Analyse professionnelle de la situation
4. **Interventions/Recommandations** : Actions men√©es ou conseill√©es
5. **Suivi/Prochaines √©tapes** : Plan de suivi, RDV programm√©s

Texte √† analyser:
`,

  todos: `Tu es un assistant de transcription m√©dicale. Extrayez de cette consultation toutes les actions concr√®tes et t√¢ches √† r√©aliser :

Format de r√©ponse souhait√© :
‚Ä¢ **Actions imm√©diates** : Ce qui doit √™tre fait rapidement
‚Ä¢ **Suivi m√©dical** : RDV, examens, prescriptions √† pr√©voir
‚Ä¢ **Recommandations patient** : Conseils donn√©s au patient
‚Ä¢ **Actions administratives** : Courriers, certificats, rapports √† r√©diger

Soyez pr√©cis et actionnable. Indiquez les √©ch√©ances quand elles sont mentionn√©es.

Texte √† analyser:
`,

  notes: `Tu es un assistant de transcription m√©dicale. Structurez cette consultation selon le format de notes th√©rapeutiques :

**1. COMPORTEMENT & PR√âSENTATION**
- √âtat g√©n√©ral du patient
- Communication verbale/non-verbale
- Humeur et attitude

**2. √âVOLUTION & PROGR√àS**  
- Changements depuis la derni√®re s√©ance
- Am√©lioration ou d√©t√©rioration observ√©e
- R√©ponse aux interventions pr√©c√©dentes

**3. INTERVENTIONS & TECHNIQUES**
- Approches th√©rapeutiques utilis√©es
- Exercices ou techniques appliqu√©es
- R√©actions du patient

**4. PLAN TH√âRAPEUTIQUE**
- Objectifs de travail d√©finis
- Strat√©gies pour les prochaines s√©ances
- Adaptations du traitement si n√©cessaire

Texte √† analyser:
`,

  custom: `Tu es un assistant de transcription m√©dicale. Analysez le texte suivant selon les instructions sp√©cifiques fournies :

{custom_instructions}

Texte √† analyser:
`
};

/**
 * V√©rifie si le bridge LLM local est disponible
 */
export async function pingLLMBridge(): Promise<boolean> {
  try {
    const response = await fetch(`${LLM_BRIDGE_URL}/status`, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
    });
    return response.ok;
  } catch (error) {
    return false;
  }
}

/**
 * R√©cup√®re le statut d√©taill√© du bridge LLM
 */
export async function getLLMBridgeStatus(): Promise<LLMBridgeStatus> {
  const preferences = JSON.parse(localStorage.getItem('llm_preferences') || '{}');
  const backend = preferences.backend || 'disabled';

  console.log('üîç LLM Bridge Status Check:', { backend, preferences });

  if (backend === 'disabled') {
    console.log('‚ùå LLM backend is disabled');
    return {
      isConnected: false,
      backend: 'disabled',
      models: [],
      error: 'LLM backend is disabled'
    };
  }

  try {
    let url: string;
    let response: Response;

    if (backend === 'ollama') {
      url = preferences.ollamaUrl || 'http://localhost:11434';
      const testUrl = `${url}/api/tags`;
      console.log('üîó Testing Ollama connection:', testUrl);
      
      response = await fetch(testUrl, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: AbortSignal.timeout(5000)
      });

      console.log('üì° Ollama response status:', response.status, response.statusText);

      if (!response.ok) {
        const errorText = await response.text().catch(() => 'No response body');
        console.log('‚ùå Ollama error response:', errorText);
        throw new Error(`Ollama API error: ${response.status} ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      console.log('‚úÖ Ollama models found:', data.models?.length || 0);
      
      return {
        isConnected: true,
        backend: 'ollama',
        models: data.models?.map((model: any) => model.name) || [],
        url: testUrl
      };
    } else if (backend === 'lmstudio') {
      url = preferences.lmstudioUrl || 'http://localhost:1234';
      const testUrl = `${url}/v1/models`;
      console.log('üîó Testing LM Studio connection:', testUrl);
      
      response = await fetch(testUrl, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
          ...(preferences.apiKey && { 'Authorization': `Bearer ${preferences.apiKey}` })
        },
        signal: AbortSignal.timeout(5000)
      });

      console.log('üì° LM Studio response status:', response.status, response.statusText);

      if (!response.ok) {
        const errorText = await response.text().catch(() => 'No response body');
        console.log('‚ùå LM Studio error response:', errorText);
        throw new Error(`LM Studio API error: ${response.status} ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      console.log('‚úÖ LM Studio models found:', data.data?.length || 0);
      
      return {
        isConnected: true,
        backend: 'lmstudio',
        models: data.data?.map((model: any) => model.id) || [],
        url: testUrl
      };
    }

    console.log('‚ùå Unsupported backend:', backend);
    return {
      isConnected: false,
      backend,
      models: [],
      error: `Backend ${backend} not supported`
    };
  } catch (error) {
    console.error('‚ùå LLM Bridge Connection Error:', error);
    
    let errorMessage = 'Unknown error occurred';
    if (error instanceof Error) {
      errorMessage = error.message;
      
      // More specific error messages based on common issues
      if (error.name === 'TypeError' && error.message.includes('fetch')) {
        errorMessage = `Cannot connect to ${backend} service. Is it running?`;
      } else if (error.name === 'AbortError' || error.message.includes('timeout')) {
        errorMessage = `Connection timeout to ${backend} service`;
      } else if (error.message.includes('ECONNREFUSED')) {
        errorMessage = `${backend} service is not accessible. Check if it's running on the configured port.`;
      }
    }
    
    return {
      isConnected: false,
      backend,
      models: [],
      error: errorMessage
    };
  }
}

/**
 * G√©n√®re un r√©sum√© structur√© de la transcription
 */
export async function generateSummary(
  transcript: string,
  options: {
    model?: string;
    onProgress?: (chunk: string) => void;
  } = {}
): Promise<string> {
  const { model = 'llama3.1:8b', onProgress } = options;
  
  return await generateAnalysis(transcript, 'summary', { model, onProgress });
}

/**
 * Extrait les t√¢ches et actions de la transcription
 */
export async function extractTodos(
  transcript: string,
  options: {
    model?: string;
    onProgress?: (chunk: string) => void;
  } = {}
): Promise<string> {
  const { model = 'llama3.1:8b', onProgress } = options;
  
  return await generateAnalysis(transcript, 'todos', { model, onProgress });
}

/**
 * G√©n√®re des notes th√©rapeutiques structur√©es
 */
export async function generateNotes(
  transcript: string,
  options: {
    model?: string;
    onProgress?: (chunk: string) => void;
  } = {}
): Promise<string> {
  const { model = 'llama3.1:8b', onProgress } = options;
  
  return await generateAnalysis(transcript, 'notes', { model, onProgress });
}

/**
 * G√©n√®re une analyse personnalis√©e de la transcription
 */
export async function generateCustomAnalysis(
  transcript: string,
  customInstructions: string,
  options: {
    model?: string;
    onProgress?: (chunk: string) => void;
  } = {}
): Promise<string> {
  const { model = 'llama3.1:8b', onProgress } = options;
  
  const prompt = PROMPT_TEMPLATES.custom.replace('{custom_instructions}', customInstructions);
  
  return await callLLMBridge(transcript, prompt, { model, onProgress });
}

/**
 * G√©n√®re une analyse bas√©e sur un type pr√©d√©fini
 */
export async function generateAnalysis(
  transcript: string,
  type: Exclude<AnalysisType, 'custom'>,
  options: {
    model?: string;
    onProgress?: (chunk: string) => void;
  } = {}
): Promise<string> {
  const { model = 'llama3.1:8b', onProgress } = options;
  
  const prompt = PROMPT_TEMPLATES[type];
  
  return await callLLMBridge(transcript, prompt, { model, onProgress });
}

/**
 * Appel principal au bridge LLM avec streaming support
 */
async function callLLMBridge(
  transcript: string,
  systemPrompt: string,
  options: {
    model?: string;
    onProgress?: (chunk: string) => void;
  } = {}
): Promise<string> {
  const { model = 'llama3.1:8b', onProgress } = options;
  
  // V√©rifier que le service LLM est disponible
  const status = await getLLMBridgeStatus();
  if (!status || !status.isConnected) {
    throw new Error('Service LLM non disponible. Veuillez d√©marrer Ollama ou LM Studio.');
  }
  
  const request: LLMRequest = {
    model,
    messages: [
      {
        role: 'system',
        content: 'Tu es un assistant de transcription m√©dicale expert. R√©ponds de mani√®re structur√©e et professionnelle.'
      },
      {
        role: 'user',
        content: systemPrompt + '\n\n' + transcript
      }
    ],
    temperature: 0.3,
    stream: !!onProgress,
    max_tokens: 2000
  };
  
  try {
    // Get preferences to determine the correct URL
    const saved = localStorage.getItem('llm_preferences');
    let preferences: {
      backend: 'ollama' | 'lmstudio' | 'disabled';
      ollamaUrl: string;
      lmstudioUrl: string;
    } = {
      backend: 'ollama',
      ollamaUrl: 'http://localhost:11434',
      lmstudioUrl: 'http://localhost:1234'
    };
    
    if (saved) {
      try {
        const parsed = JSON.parse(saved);
        preferences = { ...preferences, ...parsed };
      } catch (error) {
        console.error('Error parsing LLM preferences:', error);
      }
    }
    
    // Determine the correct endpoint
    let endpoint = '';
    if (preferences.backend === 'ollama') {
      endpoint = `${preferences.ollamaUrl}/v1/chat/completions`;
    } else if (preferences.backend === 'lmstudio') {
      endpoint = `${preferences.lmstudioUrl}/v1/chat/completions`;
    } else {
      throw new Error('Backend LLM non configur√©');
    }
    
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(request),
    });
    
    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Erreur LLM Bridge (${response.status}): ${errorText}`);
    }
    
    if (onProgress && response.body) {
      // Mode streaming
      return await handleStreamingResponse(response, onProgress);
    } else {
      // Mode non-streaming
      const result: LLMResponse = await response.json();
      return result.choices[0]?.message?.content || '';
    }
    
  } catch (error) {
    console.error('Error calling LLM bridge:', error);
    throw error;
  }
}

/**
 * G√®re la r√©ponse en streaming du LLM
 */
async function handleStreamingResponse(
  response: Response,
  onProgress: (chunk: string) => void
): Promise<string> {
  const reader = response.body!.getReader();
  const decoder = new TextDecoder();
  let fullContent = '';
  
  try {
    while (true) {
      const { done, value } = await reader.read();
      
      if (done) break;
      
      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n');
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          
          if (data === '[DONE]') {
            break;
          }
          
          try {
            const parsed: LLMStreamChunk = JSON.parse(data);
            const content = parsed.choices[0]?.delta?.content;
            
            if (content) {
              fullContent += content;
              onProgress(content);
            }
          } catch (e) {
            // Ignore les lignes JSON invalides
          }
        }
      }
    }
  } finally {
    reader.releaseLock();
  }
  
  return fullContent;
}

/**
 * R√©cup√®re les mod√®les disponibles via le bridge
 */
export async function getAvailableLLMModels(): Promise<string[]> {
  try {
    const status = await getLLMBridgeStatus();
    return status?.models || [];
  } catch (error) {
    console.error('Error fetching available LLM models:', error);
    return [];
  }
}

/**
 * R√©cup√®re le mod√®le par d√©faut
 */
export async function getDefaultLLMModel(): Promise<string> {
  try {
    const preferences = JSON.parse(localStorage.getItem('llm_preferences') || '{}');
    return preferences.defaultModel || 'llama3.1:8b';
  } catch (error) {
    return 'llama3.1:8b';
  }
}